{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fasttext_75.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "43GQt7Q8Ugfh",
        "colab_type": "code",
        "outputId": "cd2507ca-10b0-49f3-c090-338cb1c69444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8QPWMuBgmH8",
        "colab_type": "code",
        "outputId": "2361050e-f926-4a03-e4bd-1e5e578ba76f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "listed = drive.ListFile({'q': \"title contains '.pkl' and 'root' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "    print('title {}, id {}'.format(file['title'], file['id']))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title data_75_fasttext.pkl, id 1OcIBhCyhsqbaH1Mr7zu1zjDr_tUQKEii\n",
            "title refined_giacomo_75.pkl, id 1fAFqsIYNQWKBcV8pGzIPqKDM6Gql95v3\n",
            "title base_df.pkl, id 1cqUZDd2Wl--LdnzWpBuRn_dJt8PFm_LD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MzU7xjfgz86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "import io\n",
        "import pickle\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "file_id = '1cqUZDd2Wl--LdnzWpBuRn_dJt8PFm_LD'\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloaded = io.BytesIO()\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    _, done = downloader.next_chunk()\n",
        "\n",
        "downloaded.seek(0)\n",
        "f = pickle.load(downloaded)\n",
        "base_df=f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pOeiEtHiQPr",
        "colab_type": "code",
        "outputId": "11af0ca2-6df9-4cff-da8e-01a37c26f437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "base_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>__label__</th>\n",
              "      <th>tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>__label__Right</td>\n",
              "      <td>RT @luigivanti: Il problema non si risolve rin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>__label__Left</td>\n",
              "      <td>@Ire_Lica @salvinimi @DarioSegreto_ @maddale...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>__label__Unknown</td>\n",
              "      <td>Un #cinema per un popolo che manca. @TheIris...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>__label__Right</td>\n",
              "      <td>b'RT @petergomezblog: Da gioved\\xc3\\xac 5 dice...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>__label__Left</td>\n",
              "      <td>RT @RaiStoria: La Gran Bretagna è il paese p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          __label__                                             tweets\n",
              "0    __label__Right  RT @luigivanti: Il problema non si risolve rin...\n",
              "1     __label__Left    @Ire_Lica @salvinimi @DarioSegreto_ @maddale...\n",
              "2  __label__Unknown    Un #cinema per un popolo che manca. @TheIris...\n",
              "3    __label__Right  b'RT @petergomezblog: Da gioved\\xc3\\xac 5 dice...\n",
              "4     __label__Left    RT @RaiStoria: La Gran Bretagna è il paese p..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBRXtU16Otsk",
        "colab_type": "code",
        "outputId": "039aa747-8e27-449b-a2fc-089b8c98ba47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!pip install fasttext -f https://github.com/facebookresearch/fastText.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://github.com/facebookresearch/fastText.git\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.5)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6Sj4IuMl-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxmAX860MsOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_df.head()\n",
        "refined_df = base_df\n",
        "control_df = base_df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDDOkpesM01A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove html tags\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "refined_df[\"tweets\"] = base_df[\"tweets\"].apply(lambda s: BeautifulSoup(s).get_text())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY_-UdkZM15k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove hashtags and mentions\n",
        "# tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
        "import re\n",
        "refined_df[\"tweets\"] = base_df[\"tweets\"].apply(lambda s: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", s).split()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kFHpAlgM4Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove urls\n",
        "refined_df[\"tweets\"] = base_df[\"tweets\"].apply(lambda s: ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", s).split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlF6iZHDNLfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove punctuations\n",
        "refined_df[\"tweets\"] = base_df[\"tweets\"].apply(lambda s: ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\'\\...\\\"]\", \" \", s).split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKLmIhpDOweL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "refined_df[\"tweets\"] = base_df[\"tweets\"].apply(lambda s: s.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3EqPLaQO18h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove emoji since the package translates to english, too lazy to make custom\n",
        "def deEmojify(inputString):\n",
        "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "refined_df[\"tweets\"] = base_df[\"tweets\"].apply(lambda s: deEmojify(s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6PEk5D5PCrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remmove stopwords\n",
        "stop_words = ['a',\n",
        "       'b',\n",
        "       'c',\n",
        "       'd',\n",
        "       'e',\n",
        "       'f',\n",
        "       'g',\n",
        "       'h',\n",
        "       'i',\n",
        "       'l',\n",
        "       'm',\n",
        "       'n',\n",
        "       'o',\n",
        "       'p',\n",
        "       'q',\n",
        "       'r',\n",
        "       's',\n",
        "       't',\n",
        "       'u',\n",
        "       'v',\n",
        "       'z',\n",
        "       'y',\n",
        "       'w',\n",
        "       'x',\n",
        "       'j',\n",
        "       'k',\n",
        "       'Abruzzo', 'Basilicata', 'Calabria', 'Campania', 'Emilia-Romagna', 'Friuli-Venezia Giulia', 'Lazio', 'Liguria', 'Lombardia', 'Marche', 'Molisie', 'Piemonte', 'Puglia', 'Sardegna', 'Sicilia', 'Toscana', 'Trentino-Alto Adige', 'Umbria', 'Valle d Aosta', 'Veneto', \n",
        "       '2030',\n",
        "       '2029',\n",
        "       '2028',\n",
        "       '2027',\n",
        "       '2026',\n",
        "       '2025',\n",
        "       '2024',\n",
        "       '2023',\n",
        "       '2022',\n",
        "       '2021',\n",
        "       '2020',\n",
        "       '2019',\n",
        "       '2018',\n",
        "       '2017',\n",
        "       '2016',\n",
        "       '2015',\n",
        "       '2014',\n",
        "       '2013',\n",
        "       '2012',\n",
        "       '2011',\n",
        "       '2010',\n",
        "       '2009',\n",
        "       '2008',\n",
        "       '2007',\n",
        "       '2006',\n",
        "       '2005',\n",
        "       '2004',\n",
        "       '2003',\n",
        "       '2002',\n",
        "       '2001',\n",
        "       '2000',\n",
        "       '1999',\n",
        "       '1998',\n",
        "       '1997',\n",
        "       '1996',\n",
        "       '1995',\n",
        "       '1994',\n",
        "       '1993',\n",
        "       '1992',\n",
        "       '1991',\n",
        "       '1990',\n",
        "       '1989',\n",
        "       '1988',\n",
        "       '1987',\n",
        "       '1986',\n",
        "       '1985',\n",
        "       '1984',\n",
        "       '1983',\n",
        "       '1982',\n",
        "       '1981',\n",
        "       '1980',    \n",
        "  'abbia',\n",
        " 'abbiamo',\n",
        " 'abbiano',\n",
        " 'abbiate',\n",
        " 'ad',\n",
        " 'adesso',\n",
        " 'agl',\n",
        " 'agli',\n",
        " 'ai',\n",
        " 'al',\n",
        " 'all',\n",
        " 'alla',\n",
        " 'alle',\n",
        " 'allo',\n",
        " 'allora',\n",
        " 'altre',\n",
        " 'altri',\n",
        " 'altro',\n",
        " 'anche',\n",
        " 'ancora',\n",
        " 'avemmo',\n",
        " 'avendo',\n",
        " 'avere',\n",
        " 'avesse',\n",
        " 'avessero',\n",
        " 'avessi',\n",
        " 'avessimo',\n",
        " 'aveste',\n",
        " 'avesti',\n",
        " 'avete',\n",
        " 'aveva',\n",
        " 'avevamo',\n",
        " 'avevano',\n",
        " 'avevate',\n",
        " 'avevi',\n",
        " 'avevo',\n",
        " 'avrai',\n",
        " 'avranno',\n",
        " 'avrebbe',\n",
        " 'avrebbero',\n",
        " 'avrei',\n",
        " 'avremmo',\n",
        " 'avremo',\n",
        " 'avreste',\n",
        " 'avresti',\n",
        " 'avrete',\n",
        " 'avrà',\n",
        " 'avrò',\n",
        " 'avuta',\n",
        " 'avute',\n",
        " 'avuti',\n",
        " 'avuto',\n",
        " 'c',\n",
        " 'che',\n",
        " 'chi',\n",
        " 'ci',\n",
        " 'coi',\n",
        " 'col',\n",
        " 'come',\n",
        " 'con',\n",
        " 'contro',\n",
        " 'cui',\n",
        " 'da',\n",
        " 'dagl',\n",
        " 'dagli',\n",
        " 'dai',\n",
        " 'dal',\n",
        " 'dall',\n",
        " 'dalla',\n",
        " 'dalle',\n",
        " 'dallo',\n",
        " 'degl',\n",
        " 'degli',\n",
        " 'dei',\n",
        " 'del',\n",
        " 'dell',\n",
        " 'della',\n",
        " 'delle',\n",
        " 'dello',\n",
        " 'dentro',\n",
        " 'di',\n",
        " 'dov',\n",
        " 'dove',\n",
        " 'e',\n",
        " 'ebbe',\n",
        " 'ebbero',\n",
        " 'ebbi',\n",
        " 'ecco',\n",
        " 'ed',\n",
        " 'era',\n",
        " 'erano',\n",
        " 'eravamo',\n",
        " 'eravate',\n",
        " 'eri',\n",
        " 'ero',\n",
        " 'essendo',\n",
        " 'faccia',\n",
        " 'facciamo',\n",
        " 'facciano',\n",
        " 'facciate',\n",
        " 'faccio',\n",
        " 'facemmo',\n",
        " 'facendo',\n",
        " 'facesse',\n",
        " 'facessero',\n",
        " 'facessi',\n",
        " 'facessimo',\n",
        " 'faceste',\n",
        " 'facesti',\n",
        " 'faceva',\n",
        " 'facevamo',\n",
        " 'facevano',\n",
        " 'facevate',\n",
        " 'facevi',\n",
        " 'facevo',\n",
        " 'fai',\n",
        " 'fanno',\n",
        " 'farai',\n",
        " 'faranno',\n",
        " 'fare',\n",
        " 'farebbe',\n",
        " 'farebbero',\n",
        " 'farei',\n",
        " 'faremmo',\n",
        " 'faremo',\n",
        " 'fareste',\n",
        " 'faresti',\n",
        " 'farete',\n",
        " 'farà',\n",
        " 'farò',\n",
        " 'fece',\n",
        " 'fecero',\n",
        " 'feci',\n",
        " 'fino',\n",
        " 'fosse',\n",
        " 'fossero',\n",
        " 'fossi',\n",
        " 'fossimo',\n",
        " 'foste',\n",
        " 'fosti',\n",
        " 'fra',\n",
        " 'fu',\n",
        " 'fui',\n",
        " 'fummo',\n",
        " 'furono',\n",
        " 'giù',\n",
        " 'gli',\n",
        " 'ha',\n",
        " 'hai',\n",
        " 'hanno',\n",
        " 'ho',\n",
        " 'i',\n",
        " 'il',\n",
        " 'in',\n",
        " 'io',\n",
        " 'l',\n",
        " 'la',\n",
        " 'le',\n",
        " 'lei',\n",
        " 'li',\n",
        " 'lo',\n",
        " 'loro',\n",
        " 'lui',\n",
        " 'ma',\n",
        " 'me',\n",
        " 'mi',\n",
        " 'mia',\n",
        " 'mie',\n",
        " 'miei',\n",
        " 'mio',\n",
        " 'ne',\n",
        " 'negl',\n",
        " 'negli',\n",
        " 'nei',\n",
        " 'nel',\n",
        " 'nell',\n",
        " 'nella',\n",
        " 'nelle',\n",
        " 'nello',\n",
        " 'no',\n",
        " 'noi',\n",
        " 'non',\n",
        " 'nostra',\n",
        " 'nostre',\n",
        " 'nostri',\n",
        " 'nostro',\n",
        " 'o',\n",
        " 'per',\n",
        " 'perché',\n",
        " 'però',\n",
        " 'più',\n",
        " 'pochi',\n",
        " 'poco',\n",
        " 'qua',\n",
        " 'quale',\n",
        " 'quanta',\n",
        " 'quante',\n",
        " 'quanti',\n",
        " 'quanto',\n",
        " 'quasi',\n",
        " 'quella',\n",
        " 'quelle',\n",
        " 'quelli',\n",
        " 'quello',\n",
        " 'questa',\n",
        " 'queste',\n",
        " 'questi',\n",
        " 'questo',\n",
        " 'qui',\n",
        " 'quindi',\n",
        " 'sarai',\n",
        " 'saranno',\n",
        " 'sarebbe',\n",
        " 'sarebbero',\n",
        " 'sarei',\n",
        " 'saremmo',\n",
        " 'saremo',\n",
        " 'sareste',\n",
        " 'saresti',\n",
        " 'sarete',\n",
        " 'sarà',\n",
        " 'sarò',\n",
        " 'se',\n",
        " 'sei',\n",
        " 'senza',\n",
        " 'si',\n",
        " 'sia',\n",
        " 'siamo',\n",
        " 'siano',\n",
        " 'siate',\n",
        " 'siete',\n",
        " 'sono',\n",
        " 'sopra',\n",
        " 'sotto',\n",
        " 'sta',\n",
        " 'stai',\n",
        " 'stando',\n",
        " 'stanno',\n",
        " 'starai',\n",
        " 'staranno',\n",
        " 'stare',\n",
        " 'starebbe',\n",
        " 'starebbero',\n",
        " 'starei',\n",
        " 'staremmo',\n",
        " 'staremo',\n",
        " 'stareste',\n",
        " 'staresti',\n",
        " 'starete',\n",
        " 'starà',\n",
        " 'starò',\n",
        " 'stava',\n",
        " 'stavamo',\n",
        " 'stavano',\n",
        " 'stavate',\n",
        " 'stavi',\n",
        " 'stavo',\n",
        " 'stemmo',\n",
        " 'stesse',\n",
        " 'stessero',\n",
        " 'stessi',\n",
        " 'stessimo',\n",
        " 'stesso',\n",
        " 'steste',\n",
        " 'stesti',\n",
        " 'stette',\n",
        " 'stettero',\n",
        " 'stetti',\n",
        " 'stia',\n",
        " 'stiamo',\n",
        " 'stiano',\n",
        " 'stiate',\n",
        " 'sto',\n",
        " 'su',\n",
        " 'sua',\n",
        " 'sue',\n",
        " 'sugl',\n",
        " 'sugli',\n",
        " 'sui',\n",
        " 'sul',\n",
        " 'sull',\n",
        " 'sulla',\n",
        " 'sulle',\n",
        " 'sullo',\n",
        " 'suo',\n",
        " 'suoi',\n",
        " 'te',\n",
        " 'ti',\n",
        " 'tra',\n",
        " 'tu',\n",
        " 'tua',\n",
        " 'tue',\n",
        " 'tuo',\n",
        " 'tuoi',\n",
        " 'tutti',\n",
        " 'tutto',\n",
        " 'un',\n",
        " 'una',\n",
        " 'uno',\n",
        " 'vai',\n",
        " 'vi',\n",
        " 'voi',\n",
        " 'vostra',\n",
        " 'vostre',\n",
        " 'vostri',\n",
        " 'vostro',\n",
        " 'è',\n",
        "'doppo',\n",
        "'dopo',\n",
        "    'rt',\n",
        "    'b',\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "\n",
        "\n",
        "# Code to remove noisy words from a text\n",
        "\n",
        "def _remove_noise(input_text):\n",
        "    words = input_text.lower()\n",
        "    words = words.split()\n",
        "    noise_free_words = [word for word in words if word not in stop_words] \n",
        "    noise_free_text = \" \".join(noise_free_words) \n",
        "    return noise_free_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Feh6MBjPPVQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remmove stopwords\n",
        "refined_df[\"tweets\"] = base_df[\"tweets\"].apply(lambda s: _remove_noise(s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSfXFDGdK1ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove unknowns profile\n",
        "refined_df=refined_df[refined_df.loc[:,'__label__']!=\"__label__Unknown\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrjR-tASawye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_names = ['__label__',\"tweets\"]\n",
        "train_df = pd.DataFrame(columns = col_names)\n",
        "test_df = pd.DataFrame(columns = col_names)\n",
        "train_df = refined_df.iloc[:1760,:]\n",
        "test_df = refined_df.iloc[1760:,:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyMf8n11adD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "train_df['tweets']= train_df['tweets'].replace('\\n',' ', regex=True).replace('\\t',' ', regex=True)\n",
        "test_df['tweets']= test_df['tweets'].replace('\\n',' ', regex=True).replace('\\t',' ', regex=True)\n",
        "train_df.to_csv('train.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \") \n",
        "test_df.to_csv('test.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vMl5KucuHg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['tweets']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItuQbkSkefOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fasttext\n",
        "model = fasttext.train_supervised(input=\"train.txt\",epoch=50)\n",
        "model.save_model(\"model_twitter.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wL25KsAVSUV",
        "colab_type": "code",
        "outputId": "7685a7d7-ab12-4d48-81f8-a38368b10592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.predict(\"dobbiamo favorire le unioni civile siamo nel 2020\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('__label__Right',), array([0.9999429]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuyg0szzVcqf",
        "colab_type": "code",
        "outputId": "561fc58d-64a0-4d37-e535-beee3c078e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.test(\"test.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(440, 0.8159090909090909, 0.8159090909090909)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}