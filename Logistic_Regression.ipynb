{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code used for the Logistic Regression.\n",
    "We used this code to analyze both the users with 100% agreement and 75% of agreement.\n",
    "To analyze them it's enought to import the right dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Download a file based on its file ID.\n",
    "#\n",
    "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
    "listed = drive.ListFile({'q': \"title contains '.pkl' and 'root' in parents\"}).GetList()\n",
    "for file in listed:\n",
    "    print('title {}, id {}'.format(file['title'], file['id']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to import left-right dataframe\n",
    "from googleapiclient.discovery import build\n",
    "drive_service = build('drive', 'v3')\n",
    "\n",
    "import io\n",
    "import pickle\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "file_id = '1fo-gIx5b8J9eiZcMFik9zK7OTCgcIJnT'\n",
    "\n",
    "request = drive_service.files().get_media(fileId=file_id)\n",
    "downloaded = io.BytesIO()\n",
    "downloader = MediaIoBaseDownload(downloaded, request)\n",
    "done = False\n",
    "while done is False:\n",
    "    # _ is a placeholder for a progress object that we ignore.\n",
    "    # (Our file is small, so we skip reporting progress.)\n",
    "    _, done = downloader.next_chunk()\n",
    "\n",
    "downloaded.seek(0)\n",
    "f=pickle.load(downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m5s=0, sx=1, dx=2\n",
    "#I change the \n",
    "for i in np.arange(0,3300) :\n",
    "    if data.loc[i,'__label__']=='__label__Unknown' : data.loc[i,'__label__']=0\n",
    "    elif data.loc[i,'__label__']=='__label__Left' : data.loc[i,'__label__']=1\n",
    "    elif data.loc[i,'__label__']=='__label__Right' : data.loc[i,'__label__']=2\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=f\n",
    "df=df[df.loc[:,'__label__']!='__label__Unknown']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "training_data, testing_data = train_test_split(df,random_state = 2000)\n",
    "Y_train=training_data['__label__']\n",
    "Y_test=testing_data['__label__']\n",
    "X_train=training_data['tweets']\n",
    "X_test=testing_data['tweets']\n",
    "field='tweets'\n",
    "field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df,field,training_data,testing_data,type=\"binary\"):\n",
    "  if \"binary\" in type:\n",
    "        # BINARY FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=True, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=cv.transform(training_data[field].values)\n",
    "        test_feature_set=cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set, test_feature_set, cv\n",
    "   \n",
    "  elif \"counts\" in type:\n",
    "        # COUNT BASED FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=False, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=cv.transform(training_data[field].values)\n",
    "        test_feature_set=cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,cv\n",
    "    \n",
    "  else:    \n",
    "        \n",
    "        # TF-IDF BASED FEATURE REPRESENTATION\n",
    "        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "        tfidf_vectorizer.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n",
    "        test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,tfidf_vectorizer\n",
    "  \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,feature_transformer = extract_features(df, field,training_data,testing_data, type=\"counts\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=scikit_log_reg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
