{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"SVM_m5s_classifier.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"6w3kNsvHojHq","colab_type":"text"},"source":["This code is used to classify the M5S users, training the algorithm with label left and right users. "]},{"cell_type":"code","metadata":{"id":"RhCUIGLL08PB","colab_type":"code","outputId":"8af26b8f-4689-4273-ecb5-0f77a2f84f82","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Install the PyDrive wrapper & import libraries.\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Download a file based on its file ID.\n","listed = drive.ListFile({'q': \"title contains '.pkl' and 'root' in parents\"}).GetList()\n","for file in listed:\n","    print('title {}, id {}'.format(file['title'], file['id']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["title data_m5s_cleaned.pkl, id 1ibuI1n_LfAkZSzzXHIA5GKXmARyTO0de\n","title data_75_fasttext.pkl, id 1i4vviPsX50thW6KTg_x8p41BlmvAkng5\n","title refined_giacomo_100.pkl, id 107988YA16LXnv_b1cTkyf8-wqFlVnMsB\n","title refined_giacomo_75.pkl, id 1Uxbiwkwt3Wt1vH-vZfD6k36J8SMBJHrz\n","title refined1.pkl, id 1_-xtFh-iPf2-prHJeNZnOuR_y0WZlPLZ\n","title base_df.pkl, id 1Q1HYn8Xgp0azDuzFoSmRggg3GcWuizF3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HExvIYIk09it","colab_type":"code","colab":{}},"source":["from googleapiclient.discovery import build\n","drive_service = build('drive', 'v3')\n","\n","import io\n","import pickle\n","from googleapiclient.http import MediaIoBaseDownload\n","\n","file_id = '107988YA16LXnv_b1cTkyf8-wqFlVnMsB'\n","\n","request = drive_service.files().get_media(fileId=file_id)\n","downloaded = io.BytesIO()\n","downloader = MediaIoBaseDownload(downloaded, request)\n","done = False\n","while done is False:\n","    # _ is a placeholder for a progress object that we ignore.\n","    # (Our file is small, so we skip reporting progress.)\n","    _, done = downloader.next_chunk()\n","\n","downloaded.seek(0)\n","f = pickle.load(downloaded)\n","df = f"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DuuTt4i1TV7","colab_type":"code","colab":{}},"source":["from googleapiclient.discovery import build\n","drive_service = build('drive', 'v3')\n","\n","import io\n","import pickle\n","from googleapiclient.http import MediaIoBaseDownload\n","\n","file_id = '1ibuI1n_LfAkZSzzXHIA5GKXmARyTO0de'\n","\n","request = drive_service.files().get_media(fileId=file_id)\n","downloaded = io.BytesIO()\n","downloader = MediaIoBaseDownload(downloaded, request)\n","done = False\n","while done is False:\n","    # _ is a placeholder for a progress object that we ignore.\n","    # (Our file is small, so we skip reporting progress.)\n","    _, done = downloader.next_chunk()\n","\n","downloaded.seek(0)\n","h = pickle.load(downloaded)\n","df_5s = h"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uccRCix50wJM","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import re\n","df[\"tweets\"]=df[\"tweets\"].apply(lambda s: ' '.join(re.sub(\"rt\", \"\", s).split()))\n","df_5s[\"tweets\"]=df_5s[\"tweets\"].apply(lambda s: ' '.join(re.sub(\"rt\", \"\", s).split()))\n","import pandas as pd\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.preprocessing import LabelEncoder\n","from collections import defaultdict\n","from nltk.corpus import wordnet as wn\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import model_selection, naive_bayes, svm\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSvJtEyP0wJY","colab_type":"code","colab":{}},"source":["#  Create a list of of lists containing the words for each tweet for the two groups\n","words_in_tweet_rl = [tweet.split() for tweet in df['tweets']]\n","words_in_tweet_5s = [tweet.split() for tweet in df_5s['tweets']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"094mptWe0wJe","colab_type":"code","colab":{}},"source":["# libraries to go on..\n","import itertools\n","import collections\n","# List of all words across tweets\n","all_words_rl = list(itertools.chain(*words_in_tweet_rl))\n","all_words_5s = list(itertools.chain(*words_in_tweet_5s))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCMk7Dx40wJj","colab_type":"code","colab":{}},"source":["tot_voc=list(set(all_words_rl) & set(all_words_5s))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9ztuwA80wJm","colab_type":"code","colab":{}},"source":["def keep_rl(input_text):\n","    words = input_text.split()\n","    noise_free_words = [word for word in words if word in tot_voc] \n","    noise_free_text = \" \".join(noise_free_words) \n","    return noise_free_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTfNvuyR0wJq","colab_type":"code","colab":{}},"source":["df[\"tweets\"] = df[\"tweets\"].apply(lambda s: keep_rl(s))\n","df_5s[\"tweets\"] = df_5s[\"tweets\"].apply(lambda s: keep_rl(s))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yi8DtUSl0wJs","colab_type":"code","colab":{}},"source":["Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df['tweets'],df['__label__'],test_size=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTRgl8mc0wJu","colab_type":"code","colab":{}},"source":["Encoder = LabelEncoder()\n","Train_Y = Encoder.fit_transform(Train_Y)\n","Test_Y = Encoder.fit_transform(Test_Y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSMswXA90wJw","colab_type":"code","colab":{}},"source":["df[\"tweets\"] = df[\"tweets\"].apply(lambda s: str(s))\n","\n","Tfidf_vect = TfidfVectorizer(ngram_range=(1,1))\n","Tfidf_vect.fit(df['tweets'])\n","Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n","Test_X_Tfidf = Tfidf_vect.transform(Test_X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P59VY2-e0wJy","colab_type":"code","colab":{}},"source":["# Classifier - Algorithm - SVM\n","# Fit the training dataset on the classifier\n","SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n","SVM.fit(Train_X_Tfidf,Train_Y)\n","# Predict the labels on validation dataset\n","predictions_SVM = SVM.predict(Test_X_Tfidf)\n","# Use accuracy_score function to get the accuracy\n","print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"36561kGg0wJz","colab_type":"code","colab":{}},"source":["test_5s = df_5s.tweets\n","\n","Tfidf_vect1 = TfidfVectorizer(ngram_range=(1,1))\n","Tfidf_vect1.fit(df_5s['tweets'])\n","Test_5s_Tfidf = Tfidf_vect1.transform(test_5s)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmKcvRC60wJ1","colab_type":"code","colab":{}},"source":["predictions_5s = SVM.predict(Test_5s_Tfidf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOyCanSh0wJ2","colab_type":"code","colab":{}},"source":["prediction=list(predictions_5s)\n","right=prediction.count(0)\n","left=prediction.count(1)\n","right_prop=right/len(prediction)\n","left_prop=left/len(prediction)"],"execution_count":0,"outputs":[]}]}